{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы:\n",
    "* Макрушин С.В. Лекция 9: Введение в обработку текста на естественном языке\\\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy2\n",
    "\n",
    "from nltk.metrics.distance import edit_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = 'ПИ19-3'\n",
    "s2 = 'ПМ19-3'\n",
    "\n",
    "edit_distance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['высокопревосходительства',\n",
       " 'попреблагорассмотрительст',\n",
       " 'попреблагорассмотрительствующемуся',\n",
       " 'убегающих',\n",
       " 'уменьшившейся']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''\n",
    "word = 'велечайшим'\n",
    "\n",
    "with open(\"./data/litw-win.txt\", encoding=\"windows-1251\") as f:\n",
    "    words = [line.strip().split()[-1] for line in f]\n",
    "words[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(words, key=lambda w: edit_distance(w, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'попреблагорассмотрительств'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('russian')\n",
    "word = 'попреблагорассмотрительствующемуся'\n",
    "stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "попреблагорассмотрительствующийся\n",
      "ADJF masc,sing,datv\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "word = 'попреблагорассмотрительствующемуся'\n",
    "morph_word = morph.parse(word)[0]\n",
    "print(morph_word.normal_form)\n",
    "print(morph_word.tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Считайте слова из файла `litw-win.txt` и запишите их в список `words`.', 'В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`.', 'Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`.']\n",
      "[[1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0]\n",
      " [0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 2 0 0 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1]]\n",
      "{'считайте': 32, 'слова': 24, 'из': 12, 'файла': 33, 'litw': 0, 'win': 2, 'txt': 1, 'запишите': 11, 'их': 14, 'список': 31, 'words': 3, 'заданном': 9, 'предложении': 22, 'исправьте': 13, 'все': 5, 'опечатки': 21, 'заменив': 10, 'опечатками': 20, 'на': 16, 'ближайшие': 4, 'смысле': 27, 'расстояния': 23, 'левенштейна': 15, 'ним': 18, 'списка': 29, 'что': 34, 'слове': 25, 'есть': 8, 'опечатка': 19, 'если': 7, 'данное': 6, 'слово': 26, 'не': 17, 'содержится': 28, 'списке': 30}\n",
      "(3, 35)\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text = \"Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. \"\n",
    "sents = sent_tokenize(text)\n",
    "print(sents)\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(sents)\n",
    "print(X.toarray())\n",
    "# 3 предложения и 35 уникальных слов\n",
    "print(cv.vocabulary_)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>preprocessed_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>george s at the cove  black bean soup</td>\n",
       "      <td>an original recipe created by chef scott meska...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>healthy for them  yogurt popsicles</td>\n",
       "      <td>my children and their friends ask for my homem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i can t believe it s spinach</td>\n",
       "      <td>these were so go it surprised even me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>italian  gut busters</td>\n",
       "      <td>my sisterinlaw made these for us at a family g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love is in the air  beef fondue   sauces</td>\n",
       "      <td>i think a fondue is a very romantic casual din...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       name  \\\n",
       "0     george s at the cove  black bean soup   \n",
       "1        healthy for them  yogurt popsicles   \n",
       "2              i can t believe it s spinach   \n",
       "3                      italian  gut busters   \n",
       "4  love is in the air  beef fondue   sauces   \n",
       "\n",
       "                           preprocessed_descriptions  \n",
       "0  an original recipe created by chef scott meska...  \n",
       "1  my children and their friends ask for my homem...  \n",
       "2              these were so go it surprised even me  \n",
       "3  my sisterinlaw made these for us at a family g...  \n",
       "4  i think a fondue is a very romantic casual din...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "reviews = pd.read_csv(\"./data/preprocessed_descriptions.csv\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative solution\n",
    "\n",
    "# import itertools\n",
    "# res = reviews[\"preprocessed_descriptions\"].agg(lambda text: nltk.word_tokenize(str(text)))\n",
    "# result = list(itertools.chain(*res))\n",
    "# len(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072517\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rev = []\n",
    "for text in reviews[\"preprocessed_descriptions\"]:\n",
    "    rev.extend(nltk.word_tokenize(str(text)))\n",
    "print(len(rev)) #1072517\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30868\n"
     ]
    }
   ],
   "source": [
    "words = set()\n",
    "for r in rev:\n",
    "    words.add(r.lower())\n",
    "print(len(words)) #30868"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Список слов: ['awesomehence', 've', 'tastelessness', 'tartlets', 'freezerpantry', 'zereshk', '167664', 'orient', 'mono', 'them']\n",
      "\n",
      "Слово №1             Слово №2        Расстояние\n",
      "them                 mono                4     \n",
      "orient               167664              6     \n",
      "zereshk              freezerpantry       10    \n",
      "tartlets             tastelessness       7     \n",
      "ve                   awesomehence        11    \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "pairs = 5\n",
    "choise = random.sample(list(words), pairs*2)\n",
    "print(f\"Список слов: {choise}\\n\")\n",
    "\n",
    "print(f\"{'Слово №1':20} {'Слово №2':15} Расстояние\")\n",
    "for i in range(pairs):\n",
    "    word_1, word_2 = choise.pop(), choise.pop()\n",
    "    print(f\"{word_1:20} {word_2:15} {edit_distance(word_1, word_2):^10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values(['words', 'woud', 'ward', 'ford', 'wore'])\n"
     ]
    }
   ],
   "source": [
    "def nearest_words(word, k):\n",
    "    d = {}\n",
    "    for item in words:\n",
    "        d[item] = edit_distance(word, item)\n",
    "        \n",
    "    d.pop(word)\n",
    "\n",
    "    min_d = {}\n",
    "    for i in range(k):\n",
    "        min_d[i] = min(d, key=d.get)\n",
    "        d.pop(min_d[i])\n",
    "    return min_d.values()\n",
    "\n",
    "\n",
    "print(nearest_words('word', 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для лемматизации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             stemmed_word normalize_word\n",
      "mandell            mandel        mandell\n",
      "gasp                 gasp           gasp\n",
      "dram                 dram           dram\n",
      "meltedcheese  meltedchees   meltedcheese\n",
      "sweaty             sweati         sweaty\n",
      "receive            receiv        receive\n",
      "greenspan       greenspan      greenspan\n",
      "stewpot           stewpot        stewpot\n",
      "mindand           mindand        mindand\n",
      "stained             stain        stained\n",
      "impossibly         imposs     impossibly\n",
      "hotsweet         hotsweet       hotsweet\n",
      "hotpot             hotpot         hotpot\n",
      "bachs                bach           bach\n",
      "healthied         healthi      healthied\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "df = pd.DataFrame(index=words)\n",
    "df['stemmed_word'] = [stemmer.stem(word) for word in words]\n",
    "df['normalize_word'] = [lemmer.lemmatize(word) for word in words]\n",
    "\n",
    "print(df[9:24])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю от общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072517\n",
      "582574\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "words_not_uniq = []\n",
    "for text in reviews[\"preprocessed_descriptions\"]:\n",
    "    words_not_uniq.extend(nltk.word_tokenize(str(text)))\n",
    "\n",
    "\n",
    "print(len(words_not_uniq))\n",
    "\n",
    "stw_list = stopwords.words(\"english\")\n",
    "without_sw = [i for i in words_not_uniq if not i in stw_list]\n",
    "\n",
    "print(len(without_sw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля: 0.4568\n",
      "all_words = [('the', 40210), ('a', 34994), ('and', 30279), ('this', 27048), ('i', 25111), ('to', 23499), ('is', 20290), ('it', 19863), ('of', 18372), ('for', 15988)]\n",
      "without_stop = [('recipe', 14957), ('make', 6353), ('time', 5180), ('use', 4635), ('great', 4453), ('like', 4175), ('easy', 4175), ('one', 3886), ('good', 3820), ('made', 3814)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(f\"Доля: {round((len(words_not_uniq)-len(without_sw))/len(words_not_uniq), 4)}\")\n",
    "all_words = Counter(words_not_uniq).most_common(10)\n",
    "without_stop = Counter(without_sw).most_common(10)\n",
    "print(f\"{all_words = }\")\n",
    "print(f\"{without_stop = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = reviews.fillna(\"\").sample(5)\n",
    "headers = review[\"name\"].to_list()\n",
    "text = review[\"preprocessed_descriptions\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 134)\n",
      "[[0.         0.         0.         0.14273979 0.16083421 0.\n",
      "  0.         0.         0.14273979 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14273979 0.         0.         0.14273979 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14273979 0.         0.         0.         0.14273979\n",
      "  0.         0.         0.11516156 0.14273979 0.         0.\n",
      "  0.         0.         0.14273979 0.42821937 0.         0.\n",
      "  0.         0.         0.         0.         0.24125132 0.46064625\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14273979 0.\n",
      "  0.         0.         0.         0.         0.14273979 0.\n",
      "  0.         0.         0.         0.         0.08041711 0.\n",
      "  0.14273979 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11516156 0.         0.         0.\n",
      "  0.         0.14273979 0.         0.         0.14273979 0.\n",
      "  0.         0.         0.11516156 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14273979 0.14273979\n",
      "  0.         0.16083421 0.         0.         0.         0.\n",
      "  0.         0.14273979 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.28547958 0.         0.         0.14273979 0.\n",
      "  0.14273979 0.        ]\n",
      " [0.         0.13369707 0.10786595 0.         0.0753226  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.13369707 0.13369707 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.13369707 0.         0.         0.         0.\n",
      "  0.13369707 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.10786595 0.         0.13369707 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.10786595 0.10786595 0.0753226  0.2157319\n",
      "  0.         0.13369707 0.13369707 0.13369707 0.13369707 0.\n",
      "  0.13369707 0.         0.         0.13369707 0.         0.13369707\n",
      "  0.13369707 0.         0.         0.4010912  0.         0.\n",
      "  0.         0.10786595 0.         0.         0.0753226  0.\n",
      "  0.         0.         0.13369707 0.         0.         0.\n",
      "  0.13369707 0.13369707 0.         0.13369707 0.13369707 0.\n",
      "  0.         0.         0.13369707 0.         0.         0.\n",
      "  0.13369707 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.13369707 0.         0.\n",
      "  0.35815394 0.0753226  0.         0.         0.         0.26861545\n",
      "  0.         0.         0.         0.13369707 0.         0.\n",
      "  0.2157319  0.         0.         0.         0.         0.13369707\n",
      "  0.         0.         0.13369707 0.         0.         0.13369707\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.70710678 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.70710678 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.09020551 0.         0.07277724 0.         0.15246064 0.09020551\n",
      "  0.07277724 0.09020551 0.         0.18041103 0.09020551 0.\n",
      "  0.         0.         0.         0.09020551 0.09020551 0.\n",
      "  0.09020551 0.         0.         0.         0.         0.09020551\n",
      "  0.09020551 0.         0.09020551 0.18041103 0.18041103 0.\n",
      "  0.         0.         0.07277724 0.         0.09020551 0.\n",
      "  0.09020551 0.09020551 0.         0.         0.         0.09020551\n",
      "  0.09020551 0.         0.         0.         0.09020551 0.\n",
      "  0.09020551 0.09020551 0.07277724 0.07277724 0.05082021 0.\n",
      "  0.36082205 0.         0.         0.         0.         0.09020551\n",
      "  0.         0.         0.09020551 0.         0.         0.\n",
      "  0.         0.         0.09020551 0.         0.         0.27061654\n",
      "  0.09020551 0.         0.         0.09020551 0.05082021 0.09020551\n",
      "  0.         0.         0.         0.09020551 0.         0.09020551\n",
      "  0.         0.         0.         0.         0.         0.09020551\n",
      "  0.09020551 0.         0.         0.09020551 0.         0.27061654\n",
      "  0.         0.27061654 0.07277724 0.09020551 0.09020551 0.09020551\n",
      "  0.         0.09020551 0.09020551 0.         0.         0.\n",
      "  0.24164673 0.10164043 0.09020551 0.09020551 0.         0.06041168\n",
      "  0.18041103 0.         0.09020551 0.         0.         0.\n",
      "  0.14555448 0.09020551 0.09020551 0.         0.         0.\n",
      "  0.09020551 0.         0.         0.09020551 0.         0.\n",
      "  0.         0.18041103]\n",
      " [0.         0.         0.         0.         0.22504702 0.\n",
      "  0.16113964 0.         0.         0.         0.         0.19972843\n",
      "  0.19972843 0.         0.         0.         0.         0.19972843\n",
      "  0.         0.         0.19972843 0.19972843 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.19972843\n",
      "  0.         0.         0.16113964 0.19972843 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.19972843 0.         0.         0.         0.19972843\n",
      "  0.         0.         0.         0.         0.11252351 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.19972843 0.         0.         0.         0.\n",
      "  0.         0.19972843 0.         0.         0.         0.\n",
      "  0.         0.16113964 0.19972843 0.         0.11252351 0.\n",
      "  0.         0.19972843 0.         0.         0.19972843 0.\n",
      "  0.         0.         0.16113964 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.19972843 0.         0.         0.         0.         0.\n",
      "  0.13376045 0.11252351 0.         0.         0.         0.26752091\n",
      "  0.         0.         0.         0.         0.19972843 0.19972843\n",
      "  0.         0.         0.         0.         0.19972843 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "# print(vectorizer.get_feature_names())\n",
    "\n",
    "X = vectorizer.fit_transform(text)\n",
    "print(X.shape)\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      rows  mom s cranberries  \\\n",
      "0                        mom s cranberries           1.000000   \n",
      "1         beer n  butter poultry injection           0.160256   \n",
      "2             peanut butter pudding  vegan           0.000000   \n",
      "3          spicy lentil and vegetable dish           0.065597   \n",
      "4  craftier than kraft macaroni and cheese           0.109045   \n",
      "\n",
      "   beer n  butter poultry injection  peanut butter pudding  vegan  \\\n",
      "0                          0.160256                           0.0   \n",
      "1                          1.000000                           0.0   \n",
      "2                          0.000000                           1.0   \n",
      "3                          0.184521                           0.0   \n",
      "4                          0.179526                           0.0   \n",
      "\n",
      "   spicy lentil and vegetable dish  craftier than kraft macaroni and cheese  \n",
      "0                         0.065597                                 0.109045  \n",
      "1                         0.184521                                 0.179526  \n",
      "2                         0.000000                                 0.000000  \n",
      "3                         1.000000                                 0.129123  \n",
      "4                         0.129123                                 1.000000  \n"
     ]
    }
   ],
   "source": [
    "pair_similarity = (X * X.T).toarray()\n",
    "\n",
    "df = pd.DataFrame(data=pair_similarity, columns=[i for i in headers])\n",
    "rows = df[\"rows\"] = [i for i in headers]\n",
    "df.drop(labels=['rows'], axis=1, inplace=True)\n",
    "df.insert(0, \"rows\", rows)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат (словами)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Комментарий:\n",
    "Чем ближе коэффициент по столбцу или строке к единице, тем больше схожесть текстов.\n",
    "<br>Единица в матрице, говорит нам о том, что текст идентичный.</br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
